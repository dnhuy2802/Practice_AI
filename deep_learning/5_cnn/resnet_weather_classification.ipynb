{"cells":[{"cell_type":"code","source":["!gdown --id 1Tmo5_FY9io63lvHA-hVb28sl3KEu07Qx"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G0t2ISKm9HVG","executionInfo":{"status":"ok","timestamp":1700421425076,"user_tz":-420,"elapsed":5587,"user":{"displayName":"Dao Ngoc Huy","userId":"17883533123420882409"}},"outputId":"c8038fc6-010b-4578-9890-66a108c38771"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["/usr/local/lib/python3.10/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n","  warnings.warn(\n","Downloading...\n","From: https://drive.google.com/uc?id=1Tmo5_FY9io63lvHA-hVb28sl3KEu07Qx\n","To: /content/img_cls_weather_dataset.zip\n","100% 613M/613M [00:04<00:00, 131MB/s] \n"]}]},{"cell_type":"code","source":["!unzip -q /content/img_cls_weather_dataset.zip"],"metadata":{"id":"VY1RR3iD9gqF","executionInfo":{"status":"ok","timestamp":1700421433249,"user_tz":-420,"elapsed":8183,"user":{"displayName":"Dao Ngoc Huy","userId":"17883533123420882409"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["!mv /content/weather-dataset /content/data"],"metadata":{"id":"ct1rAXTI-OTb","executionInfo":{"status":"ok","timestamp":1700421433249,"user_tz":-420,"elapsed":5,"user":{"displayName":"Dao Ngoc Huy","userId":"17883533123420882409"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","execution_count":7,"metadata":{"id":"_LaNc9zF9HWY","executionInfo":{"status":"ok","timestamp":1700421439176,"user_tz":-420,"elapsed":5931,"user":{"displayName":"Dao Ngoc Huy","userId":"17883533123420882409"}}},"outputs":[],"source":["import torch\n","import torch .nn as nn\n","import os\n","import numpy as np\n","import matplotlib . pyplot as plt\n","from PIL import Image\n","from torch.utils . data import Dataset , DataLoader\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"DlYTNjF69HWZ","executionInfo":{"status":"ok","timestamp":1700421439176,"user_tz":-420,"elapsed":8,"user":{"displayName":"Dao Ngoc Huy","userId":"17883533123420882409"}}},"outputs":[],"source":["root_dir = '/content/data/dataset'\n","\n","# get Class Names\n","classes = {\n","    label_idx : class_name for label_idx, class_name in enumerate(sorted(os.listdir(root_dir)))\n","}"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"ceFKQinQ9HWZ","executionInfo":{"status":"ok","timestamp":1700421439176,"user_tz":-420,"elapsed":7,"user":{"displayName":"Dao Ngoc Huy","userId":"17883533123420882409"}}},"outputs":[],"source":["img_paths = []\n","labels = []\n","\n","# get Image Paths and Labels\n","for label_idx, class_name in classes.items():\n","    for img_name in os.listdir(os.path.join(root_dir, class_name)):\n","        img_paths.append(os.path.join(root_dir,class_name,img_name))\n","        labels.append(label_idx)"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"q10bW31h9HWZ","executionInfo":{"status":"ok","timestamp":1700421439176,"user_tz":-420,"elapsed":7,"user":{"displayName":"Dao Ngoc Huy","userId":"17883533123420882409"}}},"outputs":[],"source":["seed = 0\n","val_size = 0.2\n","test_size = 0.125\n","is_shuffle = True\n","\n","# split train, validation, test set\n","X_train , X_val , y_train , y_val = train_test_split(\n","    img_paths,\n","    labels,\n","    test_size=test_size,\n","    random_state=seed,\n","    shuffle=is_shuffle\n",")\n","\n","X_train , X_test , y_train , y_test = train_test_split(\n","    X_train,\n","    y_train,\n","    test_size=val_size,\n","    random_state=seed,\n","    shuffle=is_shuffle\n",")"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"yXaldwPk9HWa","executionInfo":{"status":"ok","timestamp":1700421439176,"user_tz":-420,"elapsed":6,"user":{"displayName":"Dao Ngoc Huy","userId":"17883533123420882409"}}},"outputs":[],"source":["# define Dataset\n","class WeatherDataset(Dataset):\n","    def __init__(self, img_paths, labels, transform=None):\n","        self.img_paths = img_paths\n","        self.labels = labels\n","        self.transform = transform\n","\n","    def __getitem__(self, index):\n","        img_path = self.img_paths[index]\n","        label = self.labels[index]\n","        img = Image.open(img_path)\n","        img = img.convert(\"RGB\")\n","        if self.transform is not None:\n","            img = self.transform(img)\n","        return img, label\n","\n","    def __len__(self):\n","        return len(self.img_paths)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"9MptZAgq9HWa","executionInfo":{"status":"ok","timestamp":1700421439176,"user_tz":-420,"elapsed":6,"user":{"displayName":"Dao Ngoc Huy","userId":"17883533123420882409"}}},"outputs":[],"source":["# define Preprocess Images: Transform\n","def transform_image(image, image_size=(224,224)):\n","    img = image.resize(image_size)\n","    img = np.array(img)[...,:3] # in data, some image has 4 channels, but we need only 3 channels\n","    img = torch.tensor(img).permute(2,0,1).float() # permute: (H,W,C) -> (C,H,W); C is number of channels\n","    norm_img = img / 255.0\n","    return norm_img"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"-3YuW7Xb9HWa","executionInfo":{"status":"ok","timestamp":1700421439176,"user_tz":-420,"elapsed":5,"user":{"displayName":"Dao Ngoc Huy","userId":"17883533123420882409"}}},"outputs":[],"source":["train_dataset = WeatherDataset(X_train, y_train, transform = transform_image)\n","val_dataset = WeatherDataset(X_val, y_val, transform = transform_image)\n","test_dataset = WeatherDataset(X_test, y_test, transform = transform_image)"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"6YxB9_Mg9HWa","executionInfo":{"status":"ok","timestamp":1700421439177,"user_tz":-420,"elapsed":6,"user":{"displayName":"Dao Ngoc Huy","userId":"17883533123420882409"}}},"outputs":[],"source":["# Define DataLoader\n","train_batch_size = 64\n","test_batch_size = 8\n","\n","train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=test_batch_size, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False)"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"az8_3CgF9HWa","executionInfo":{"status":"ok","timestamp":1700421439177,"user_tz":-420,"elapsed":6,"user":{"displayName":"Dao Ngoc Huy","userId":"17883533123420882409"}}},"outputs":[],"source":["# Define Model: ResNet\n","class ResidualBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, stride=1):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n","        self.bn1 = nn.BatchNorm2d(out_channels)\n","        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n","        self.bn2 = nn.BatchNorm2d(out_channels)\n","\n","        self.downsample = nn.Sequential()\n","        if stride != 1 or in_channels != out_channels:\n","            self.downsample = nn.Sequential(\n","                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),\n","                nn.BatchNorm2d(out_channels)\n","            )\n","\n","        self.relu = nn.ReLU(inplace=True)\n","\n","    def forward(self, x):\n","        shortcut = x.clone()\n","        out = self.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.downsample(shortcut)\n","        out = self.relu(out)\n","        return out\n","\n","class ResNet(nn.Module):\n","    def __init__(self, residual_block, layers, num_classes=4):\n","        super().__init__()\n","        self.in_channels = 64\n","        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3) # 3 is number of channels\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","\n","        self.layer1 = self.make_layer(residual_block, 64, layers[0], stride=1)\n","        self.layer2 = self.make_layer(residual_block, 128, layers[1], stride=2)\n","        self.layer3 = self.make_layer(residual_block, 256, layers[2], stride=2)\n","        self.layer4 = self.make_layer(residual_block, 512, layers[3], stride=2)\n","\n","        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n","        self.flatten = nn.Flatten()\n","        self.fc = nn.Linear(512, num_classes)\n","\n","    def make_layer(self, residual_block, out_channels, blocks, stride=1):\n","        layers = []\n","        layers.append(residual_block(self.in_channels, out_channels, stride))\n","        self.in_channels = out_channels\n","        for _ in range(1, blocks):\n","            layers.append(residual_block(self.in_channels, out_channels, stride=1))\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        out = self.relu(self.bn1(self.conv1(x)))\n","        out = self.maxpool(out)\n","\n","        out = self.layer1(out)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        out = self.layer4(out)\n","\n","        out = self.avgpool(out) # (B, 512, 1, 1)\n","        out = self.flatten(out) # (B, 512)\n","        out = self.fc(out) # (B, 4)\n","        return out"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"RAUaCppO9HWa","executionInfo":{"status":"ok","timestamp":1700421439658,"user_tz":-420,"elapsed":486,"user":{"displayName":"Dao Ngoc Huy","userId":"17883533123420882409"}}},"outputs":[],"source":["n_classes = len(list(classes.keys()))\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","model = ResNet(ResidualBlock, [2,2,2,2], num_classes=n_classes).to(device)"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"2w23JPG19HWb","executionInfo":{"status":"ok","timestamp":1700421439659,"user_tz":-420,"elapsed":3,"user":{"displayName":"Dao Ngoc Huy","userId":"17883533123420882409"}}},"outputs":[],"source":["# Define Evaluation\n","def evaluate(model, data_loader, criterion, device):\n","    model.eval()\n","    total = 0\n","    correct = 0\n","    losses = []\n","\n","    with torch.no_grad():\n","        for data in data_loader:\n","            inputs, labels = data\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","\n","            _, predicted = torch.max(outputs, dim=1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","            losses.append(loss.item())\n","\n","    loss = sum(losses) / len(losses)\n","    acc = correct / total\n","\n","    return loss, acc"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"CvWpswRy9HWb","executionInfo":{"status":"ok","timestamp":1700421439659,"user_tz":-420,"elapsed":2,"user":{"displayName":"Dao Ngoc Huy","userId":"17883533123420882409"}}},"outputs":[],"source":["# define Training\n","def fit(model, train_loader, val_loader, criterion, optimizer, n_epochs, device):\n","    train_losses = []\n","    val_losses = []\n","\n","    for epoch in range(n_epochs):\n","        batch_train_losses = []\n","        model.train()\n","\n","        for idx, data in enumerate(train_loader):\n","            inputs, labels = data\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","\n","            optimizer.zero_grad()\n","\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            batch_train_losses.append(loss.item())\n","\n","        train_loss = sum(batch_train_losses) / len(batch_train_losses)\n","        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n","\n","        train_losses.append(train_loss)\n","        val_losses.append(val_loss)\n","\n","        print(f\"Epoch: {epoch+1}/{n_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n","\n","    return train_losses, val_losses"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"IGAXsjci9HWb","executionInfo":{"status":"ok","timestamp":1700421441115,"user_tz":-420,"elapsed":1458,"user":{"displayName":"Dao Ngoc Huy","userId":"17883533123420882409"}}},"outputs":[],"source":["# Define Training Parameters\n","lr = 1e-3\n","n_epochs = 50\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=lr)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dIhJU7Cf9HWb","outputId":"3fe50505-fc5d-4490-b5d3-ddb7fa2f55fd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1/50, Train Loss: 1.9221, Val Loss: 1.5978, Val Acc: 0.4697\n"]}],"source":["# Training\n","train_losses, val_losses = fit(model, train_loader, val_loader, criterion, optimizer, n_epochs, device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yRjaDogU9HWc"},"outputs":[],"source":["# Plot Losses\n","plt.plot(train_losses, label='Train Loss')\n","plt.plot(val_losses, label='Val Loss')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h7e5I8UA9HWc"},"outputs":[],"source":["# Define Test\n","val_loss , val_acc = evaluate (model, val_loader, criterion, device)\n","test_loss , test_acc = evaluate (model, test_loader, criterion, device )\n","\n","print ('Evaluation on val / test dataset')\n","print ('Val accuracy :', val_acc )\n","print ('Test accuracy :', test_acc)"]}],"metadata":{"kernelspec":{"display_name":"torch","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}