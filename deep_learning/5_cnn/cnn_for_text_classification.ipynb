{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jJmaFmQJILIG"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "from torchsummary import summary\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tAoT7bubJGAO"
      },
      "outputs": [],
      "source": [
        "SEED = 1\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Hqri8gZIWJfv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def load_data_from_path(folder_path):\n",
        "    examples = []\n",
        "    for label in os.listdir(folder_path):\n",
        "        full_path = os.path.join(folder_path, label)\n",
        "        for file_name in os.listdir(full_path):\n",
        "            file_path = os.path.join(full_path, file_name)\n",
        "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                lines = f.readlines()\n",
        "            sentence = \" \".join(lines)\n",
        "            if label == \"neg\":\n",
        "                label = 0\n",
        "            if label == \"pos\":\n",
        "                label = 1\n",
        "            data = {\n",
        "                'sentence': sentence,\n",
        "                'label': label\n",
        "            }\n",
        "            examples.append(data)\n",
        "    return pd.DataFrame(examples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qgUOsL2TWLPH"
      },
      "outputs": [],
      "source": [
        "folder_paths = {\n",
        "    'train': 'data/ntc-scv/train',\n",
        "    'valid': 'data/ntc-scv/valid',\n",
        "    'test': 'data/ntc-scv/test'\n",
        "}\n",
        "\n",
        "train_df = load_data_from_path(folder_paths['train'])\n",
        "valid_df = load_data_from_path(folder_paths['valid'])\n",
        "test_df = load_data_from_path(folder_paths['test'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYD88yN6WNmq"
      },
      "source": [
        "###**3.2. Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xCKYuCjgWVzv"
      },
      "outputs": [],
      "source": [
        "from langid.langid import LanguageIdentifier, model\n",
        "\n",
        "def identify_vn(df):\n",
        "    identifier = LanguageIdentifier.from_modelstring(model, norm_probs=True)\n",
        "    not_vi_idx = set()\n",
        "    THRESHOLD = 0.9\n",
        "    for idx, row in df.iterrows():\n",
        "        score = identifier.classify(row[\"sentence\"])\n",
        "        if score[0] != \"vi\" or (score[0] == \"vi\" and score[1] <= THRESHOLD):\n",
        "            not_vi_idx.add(idx)\n",
        "    vi_df = df[~df.index.isin(not_vi_idx)]\n",
        "    not_vi_df = df[df.index.isin(not_vi_idx)]\n",
        "    return vi_df, not_vi_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "x7_R7pp7WW8q"
      },
      "outputs": [],
      "source": [
        "train_df_vi, train_df_other = identify_vn(train_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "NDp2y91iWZ-P"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # remove URLs https://www.\n",
        "    url_pattern = re.compile(r'https?://\\s+\\wwww\\.\\s+')\n",
        "    text = url_pattern.sub(r\" \", text)\n",
        "\n",
        "    # remove HTML Tags: <>\n",
        "    html_pattern = re.compile(r'<[^<>]+>')\n",
        "    text = html_pattern.sub(\" \", text)\n",
        "\n",
        "    # remove puncs and digits\n",
        "    replace_chars = list(string.punctuation + string.digits)\n",
        "    for char in replace_chars:\n",
        "        text = text.replace(char, \" \")\n",
        "\n",
        "    # remove emoji\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\U0001F1F2-\\U0001F1F4\"  # Macau flag\n",
        "        u\"\\U0001F1E6-\\U0001F1FF\"  # flags\n",
        "        u\"\\U0001F600-\\U0001F64F\"\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        u\"\\U0001f926-\\U0001f937\"\n",
        "        u\"\\U0001F1F2\"\n",
        "        u\"\\U0001F1F4\"\n",
        "        u\"\\U0001F620\"\n",
        "        u\"\\u200d\"\n",
        "        u\"\\u2640-\\u2642\"\n",
        "        \"]+\", flags=re.UNICODE)\n",
        "    text = emoji_pattern.sub(r\" \", text)\n",
        "\n",
        "    # normalize whitespace\n",
        "    text = \" \".join(text.split())\n",
        "\n",
        "    # lowercasing\n",
        "    text = text.lower()\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4_LoaNXWcKz",
        "outputId": "5a7352bf-232a-4189-a24c-120d6fd35f6c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_76657/1058621333.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_df_vi['preprocess_sentence'] = [preprocess_text(row['sentence']) for index, row in train_df_vi.iterrows()]\n"
          ]
        }
      ],
      "source": [
        "train_df_vi['preprocess_sentence'] = [preprocess_text(row['sentence']) for index, row in train_df_vi.iterrows()]\n",
        "valid_df['preprocess_sentence'] = [preprocess_text(row['sentence']) for index, row in valid_df.iterrows()]\n",
        "test_df['preprocess_sentence'] = [preprocess_text(row['sentence']) for index, row in test_df.iterrows()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysfkpVN8WeTA"
      },
      "source": [
        "###**3.3. Representation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "W7C0Wu9OWhtW"
      },
      "outputs": [],
      "source": [
        "# !pip install -q torchtext==0.16.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "7F2zmrCDWjPR"
      },
      "outputs": [],
      "source": [
        "def yield_tokens(sentences, tokenizer):\n",
        "    for sentence in sentences:\n",
        "        yield tokenizer(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "jzNbnN9JWjRZ"
      },
      "outputs": [],
      "source": [
        "# word-based tokenizer\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "tokenizer = get_tokenizer(\"basic_english\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "kIf0sk6kWjTn"
      },
      "outputs": [],
      "source": [
        "# build vocabulary\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "vocab_size = 10000\n",
        "vocabulary = build_vocab_from_iterator(\n",
        "    yield_tokens(train_df_vi['preprocess_sentence'], tokenizer),\n",
        "    max_tokens=vocab_size,\n",
        "    specials=[\"<pad>\", \"<unk>\"]\n",
        ")\n",
        "vocabulary.set_default_index(vocabulary[\"<unk>\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neiHfHavY_6r",
        "outputId": "40b2a622-249a-437f-d375-a34f4e38f2ea"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocabulary[\"<pad>\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "i4_bVad3WjVp"
      },
      "outputs": [],
      "source": [
        "from torchtext.data.functional import to_map_style_dataset\n",
        "\n",
        "def prepare_dataset(df):\n",
        "    # create iterator for dataset: (sentence, label)\n",
        "    for index, row in df.iterrows():\n",
        "        sentence = row['preprocess_sentence']\n",
        "        encoded_sentence = vocabulary(tokenizer(sentence))\n",
        "        label = row['label']\n",
        "        yield encoded_sentence, label\n",
        "\n",
        "train_dataset = prepare_dataset(train_df_vi)\n",
        "train_dataset = to_map_style_dataset(train_dataset)\n",
        "\n",
        "valid_dataset = prepare_dataset(valid_df)\n",
        "valid_dataset = to_map_style_dataset(valid_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhiWKeY0YRXy"
      },
      "source": [
        "###**3.4. Dataloader**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "-mbVV_E6YXmM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def collate_batch(batch):\n",
        "    # create inputs, offsets, labels for batch\n",
        "    encoded_sentences, labels = [], []\n",
        "    for encoded_sentence, label in batch:\n",
        "        labels.append(label)\n",
        "        encoded_sentence = torch.tensor(encoded_sentence, dtype=torch.int64)\n",
        "        encoded_sentences.append(encoded_sentence)\n",
        "\n",
        "    labels = torch.tensor(labels, dtype=torch.int64)\n",
        "    encoded_sentences = pad_sequence(\n",
        "        encoded_sentences,\n",
        "        padding_value=vocabulary[\"<pad>\"]\n",
        "    )\n",
        "\n",
        "    return encoded_sentences, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "UlGlSjvtZJfY"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 128\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_batch\n",
        ")\n",
        "valid_dataloader = DataLoader(\n",
        "    valid_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_batch\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHqk8A1dZLOI",
        "outputId": "c4c7c35c-b922-4ddd-d216-613afb7ffff5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[ 142,   24,   58,  ...,   13,    6,  117],\n",
              "         [  67,  498,   96,  ...,  804,  121,   58],\n",
              "         [  66, 1243,  651,  ...,    5,   52,   56],\n",
              "         ...,\n",
              "         [   0,    0,    0,  ...,    0,    0,    0],\n",
              "         [   0,    0,    0,  ...,    0,    0,    0],\n",
              "         [   0,    0,    0,  ...,    0,    0,    0]]),\n",
              " tensor([1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,\n",
              "         1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,\n",
              "         0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
              "         0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,\n",
              "         1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,\n",
              "         1, 0, 0, 0, 1, 1, 0, 1]))"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "next(iter(train_dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "QiMlYIuAZNPY"
      },
      "outputs": [],
      "source": [
        "encoded_sentences, labels = next(iter(train_dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvgIPLMJZO57",
        "outputId": "cfa58c86-42f9-48d8-a79c-87060cf1068a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([383, 128])"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoded_sentences.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeDzx-BoZX9A"
      },
      "source": [
        "###**3.5. Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "0-8gNDBvZaKd"
      },
      "outputs": [],
      "source": [
        "class TextCNN(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size, embedding_dim, kernel_sizes, num_filters, num_classes):\n",
        "        super(TextCNN, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.kernel_sizes = kernel_sizes\n",
        "        self.num_filters = num_filters\n",
        "        self.num_classes = num_classes\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.conv = nn.ModuleList([\n",
        "            nn.Conv1d(\n",
        "                in_channels=embedding_dim,\n",
        "                out_channels=num_filters,\n",
        "                kernel_size=k,\n",
        "                stride=1\n",
        "            ) for k in kernel_sizes])\n",
        "        self.fc = nn.Linear(len(kernel_sizes) * num_filters, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, sequence_length = x.shape\n",
        "        x = self.embedding(x.T).transpose(1, 2)\n",
        "        x = [F.relu(conv(x)) for conv in self.conv]\n",
        "        x = [F.max_pool1d(c, c.size(-1)).squeeze(dim=-1) for c in x]\n",
        "        x = torch.cat(x, dim=1)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "qcV1qvGqZqoI"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(vocabulary)\n",
        "embedding_dim = 100\n",
        "\n",
        "model = TextCNN(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    kernel_sizes=[3, 4, 5],\n",
        "    num_filters=100,\n",
        "    num_classes=2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uvT6i-RaRgc",
        "outputId": "e6a2a588-fe54-40e1-d7d4-0fb483bb9c26"
      },
      "outputs": [],
      "source": [
        "predictions = model(encoded_sentences)\n",
        "predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNqGgjXJac0v"
      },
      "source": [
        "###**3.6. Loss & Optimizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAuJ2UbAabLl"
      },
      "outputs": [],
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQa3-oiAamgn",
        "outputId": "6867f611-b2ea-445b-8934-ae9dbbca552d"
      },
      "outputs": [],
      "source": [
        "loss = criterion(predictions, labels)\n",
        "loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CR-LASFrayrj"
      },
      "source": [
        "###**3.7. Trainer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fl6GnoUa1-m"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def train(model, optimizer, criterion, train_dataloader, device, epoch=0, log_interval=50):\n",
        "    model.train()\n",
        "    total_acc, total_count = 0, 0\n",
        "    losses = []\n",
        "    start_time = time.time()\n",
        "\n",
        "    for idx, (inputs, labels) in enumerate(train_dataloader):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # zero grad\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # predictions\n",
        "        predictions = model(inputs)\n",
        "\n",
        "        # compute loss\n",
        "        loss = criterion(predictions, labels)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        # backward\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_acc += (predictions.argmax(1) == labels).sum().item()\n",
        "        total_count += labels.size(0)\n",
        "        if idx % log_interval == 0 and idx > 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print(\n",
        "                \"| epoch {:3d} | {:5d}/{:5d} batches \"\n",
        "                \"| accuracy {:8.3f}\".format(\n",
        "                    epoch, idx, len(train_dataloader), total_acc / total_count\n",
        "                )\n",
        "            )\n",
        "            total_acc, total_count = 0, 0\n",
        "            start_time = time.time()\n",
        "\n",
        "    epoch_acc = total_acc / total_count\n",
        "    epoch_loss = sum(losses) / len(losses)\n",
        "    return epoch_acc, epoch_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smPG2rQtbGME",
        "outputId": "4c6f042e-fa8f-4f58-a458-6062d4f62075"
      },
      "outputs": [],
      "source": [
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIfPB_YHbKK0",
        "outputId": "52e4f039-7afe-4acf-8c95-0a8ea48d5ec7"
      },
      "outputs": [],
      "source": [
        "epoch_acc, epoch_loss = train(model, optimizer, criterion, train_dataloader, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zR4a7JWbM6m",
        "outputId": "50cd04ec-3acd-47c1-b066-943b944508ff"
      },
      "outputs": [],
      "source": [
        "epoch_acc, epoch_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CHw-xiobQnG"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, criterion, valid_dataloader):\n",
        "    model.eval()\n",
        "    total_acc, total_count = 0, 0\n",
        "    losses = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (inputs, labels) in enumerate(valid_dataloader):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            # predictions\n",
        "            predictions = model(inputs)\n",
        "\n",
        "            # compute loss\n",
        "            loss = criterion(predictions, labels)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            total_acc += (predictions.argmax(1) == labels).sum().item()\n",
        "            total_count += labels.size(0)\n",
        "\n",
        "    epoch_acc = total_acc / total_count\n",
        "    epoch_loss = sum(losses) / len(losses)\n",
        "    return epoch_acc, epoch_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_Jt2UZJbZRE"
      },
      "outputs": [],
      "source": [
        "eval_acc, eval_loss = evaluate(model, criterion, valid_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vaNLSsBJba8h",
        "outputId": "a1e5d4af-6814-4b2e-e1b5-6437da93e56e"
      },
      "outputs": [],
      "source": [
        "eval_acc, eval_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQjxsnBFbjEr"
      },
      "source": [
        "###**3.8. Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLpPpGPFbdUB",
        "outputId": "c8369bc0-f29f-4a3e-b68e-66528e76e9d3"
      },
      "outputs": [],
      "source": [
        "num_class = 2\n",
        "vocab_size = len(vocabulary)\n",
        "embedding_dim = 100\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = TextCNN(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    kernel_sizes=[3, 4, 5],\n",
        "    num_filters=100,\n",
        "    num_classes=2\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "num_epochs = 10\n",
        "save_model = './model'\n",
        "\n",
        "train_accs, train_losses = [], []\n",
        "eval_accs, eval_losses = [], []\n",
        "best_loss_eval = 100\n",
        "\n",
        "for epoch in range(1, num_epochs+1):\n",
        "    epoch_start_time = time.time()\n",
        "    # Training\n",
        "    train_acc, train_loss = train(model, optimizer, criterion, train_dataloader, device, epoch)\n",
        "    train_accs.append(train_acc)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # Evaluation\n",
        "    eval_acc, eval_loss = evaluate(model, criterion, valid_dataloader)\n",
        "    eval_accs.append(eval_acc)\n",
        "    eval_losses.append(eval_loss)\n",
        "\n",
        "    # Save best model\n",
        "    if eval_loss < best_loss_eval:\n",
        "        torch.save(model.state_dict(), save_model + '/text_cnn_model.pt')\n",
        "\n",
        "    # Print loss, acc end epoch\n",
        "    print(\"-\" * 59)\n",
        "    print(\n",
        "        \"| End of epoch {:3d} | Time: {:5.2f}s | Train Accuracy {:8.3f} | Train Loss {:8.3f} \"\n",
        "        \"| Valid Accuracy {:8.3f} | Valid Loss {:8.3f} \".format(\n",
        "            epoch, time.time() - epoch_start_time, train_acc, train_loss, eval_acc, eval_loss\n",
        "        )\n",
        "    )\n",
        "    print(\"-\" * 59)\n",
        "\n",
        "    # Load best model\n",
        "    model.load_state_dict(torch.load(save_model + '/text_cnn_model.pt'))\n",
        "    model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "LriFfSAFcqeO",
        "outputId": "1156c1c4-c824-4c42-d258-b69b096a2481"
      },
      "outputs": [],
      "source": [
        "def plot_result(num_epochs, train_accs, eval_accs, train_losses, eval_losses):\n",
        "    epochs = list(range(num_epochs))\n",
        "    fig, axs = plt.subplots(nrows = 1, ncols =2 , figsize = (12,6))\n",
        "    axs[0].plot(epochs, train_accs, label = \"Training\")\n",
        "    axs[0].plot(epochs, eval_accs, label = \"Evaluation\")\n",
        "    axs[1].plot(epochs, train_losses, label = \"Training\")\n",
        "    axs[1].plot(epochs, eval_losses, label = \"Evaluation\")\n",
        "    axs[0].set_xlabel(\"Epochs\")\n",
        "    axs[1].set_xlabel(\"Epochs\")\n",
        "    axs[0].set_ylabel(\"Accuracy\")\n",
        "    axs[1].set_ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "\n",
        "plot_result(num_epochs, train_accs, eval_accs, train_losses, eval_losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIuqLpSOc2VM"
      },
      "source": [
        "###**3.9. Evaluation & Prediction**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvizpBBlc166"
      },
      "outputs": [],
      "source": [
        "test_dataset = prepare_dataset(test_df)\n",
        "test_dataset = to_map_style_dataset(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gChIFVAQc-5R"
      },
      "outputs": [],
      "source": [
        "test_dataloader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_batch\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Tw96VDYdIX-",
        "outputId": "0d28dd39-10f0-44ea-849b-e6b1729ece15"
      },
      "outputs": [],
      "source": [
        "test_acc, test_loss = evaluate(model, criterion, valid_dataloader)\n",
        "test_acc, test_loss"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
