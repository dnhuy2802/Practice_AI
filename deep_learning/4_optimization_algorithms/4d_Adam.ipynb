{"cells":[{"cell_type":"markdown","metadata":{"id":"PDZxOtCZt5VU"},"source":["## Problem"]},{"cell_type":"markdown","metadata":{"id":"Z7esFCIXuNXS"},"source":["$$f(w_1, w_2) = 0.1w_1^2 + 2w_2^2 \\;\\;\\;\\;\\;\\;\\;(1)$$ "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_IAVg99F9N0y"},"outputs":[],"source":["import numpy as np"]},{"cell_type":"markdown","metadata":{"id":"77XQZxMK2l4x"},"source":["### Adam"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uai1hzbWuNaK"},"outputs":[],"source":["def df_w(w):\n","    \"\"\"\n","    Thực hiện tính gradient của dw1 và dw2\n","    Arguments:\n","    W -- np.array [w1, w2]\n","    Returns:\n","    dW -- np.array [dw1, dw2], array chứa giá trị đạo hàm theo w1 và w2 \n","    \"\"\"\n","    #################### YOUR CODE HERE ####################\n","    \n","\n","    dW = np.array([0.1*2*W[0], 2*2*W[1]])\n","    ########################################################\n","    \n","    return dW"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d9HnD57lC1X1"},"outputs":[],"source":["def Adam(W, dW, lr, V, S, beta1, beta2, t):\n","    \"\"\"\n","    Thực hiện thuật tóan Adam để update w1 và w2\n","    Arguments:\n","    W -- np.array: [w1, w2]\n","    dW -- np.array: [dw1, dw2], array chứa giá trị đạo hàm theo w1 và w2 \n","    lr -- float: learning rate \n","    V -- np.array: [v1, v2] Exponentially weighted averages gradients\n","    S -- np.array: [s1, s2] Exponentially weighted averages bình phương gradients\n","    beta1 -- float: hệ số long-range average cho V\n","    beta2 -- float: hệ số long-range average cho S\n","    t -- int: lần thứ t update (bắt đầu bằng 1)\n","    Returns:\n","    W -- np.array: [w1, w2] w1 và w2 sau khi đã update\n","    V -- np.array: [v1, v2] Exponentially weighted averages gradients sau khi đã cập nhật\n","    S -- np.array: [s1, s2] Exponentially weighted averages bình phương gradients sau khi đã cập nhật\n","    \"\"\"\n","    epsilon = 1e-6\n","    #################### YOUR CODE HERE ####################\n","    V = beta1*V + (1-beta1)*dW\n","    S = beta2*S + (1-beta2)*dW**2\n","    V_corrected = V/(1-beta1**t)\n","    S_corrected = S/(1-beta2**t)\n","\n","    W = W - lr*V_corrected/(np.sqrt(S_corrected)+epsilon)\n","    ########################################################\n","    return W, V, S"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DSux416mEjYy"},"outputs":[],"source":["def train_p1(optimizer, lr, epochs):\n","    \"\"\"\n","    Thực hiện tìm điểm minimum của function (1) dựa vào thuật toán \n","    được truyền vào từ optimizer\n","    Arguments:\n","    optimize : function thực hiện thuật toán optimization cụ thể\n","    lr -- float: learning rate \n","    epochs -- int: số lượng lần (epoch) lặp để tìm điểm minimum \n","    Returns:\n","    results -- list: list các cặp điểm [w1, w2] sau mỗi epoch (mỗi lần cập nhật)\n","    \"\"\"\n","    # initial\n","    W = np.array([-5, -2], dtype=np.float32)\n","    V = np.array([0, 0], dtype=np.float32)\n","    S = np.array([0, 0], dtype=np.float32)\n","    results = [W]\n","    #################### YOUR CODE HERE ####################\n","    # Tạo vòng lặp theo số lần epochs\n","    # tìm gradient dW gồm dw1 và dw2\n","    # dùng thuật toán optimization cập nhật w1, w2, s1, s2, v1, v2\n","    # append cặp [w1, w2] vào list results\n","    # các bạn lưu ý mỗi lần lặp nhớ lấy t (lần thứ t lặp) và t bất đầu bằng 1\n","\n","    for i in range(epochs):\n","        dW = df_w(W)\n","        W, V, S = optimizer(W, dW, lr, V, S, beta1=0.9, beta2=0.999, t=i+1)\n","        results.append(W)\n","\n","    \n","    ########################################################\n","    return results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uwBvH3HeEjUL"},"outputs":[],"source":["train_p1(Adam, lr=0.2, epochs=30)"]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
